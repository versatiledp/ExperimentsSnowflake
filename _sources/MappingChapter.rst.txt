.. Lines starting with two dots are special commands. But if no command can be found, the line is considered as a comment
..  rst2html.py  --stylesheet my.css    in_file.rst out_file.html

======================================================
Attribute mapping 
======================================================

We will learn the core data mapping between source system and different transformation layers
 

Academic Institution 
--------------------

Institutional Characteristics header files [HDR] are CSV format. 
These files are directly ingested into snowflake staging from source system. 
ELT layer defines the schema for CSV file with data types. 
The following diagram displays mapping between CSV attribute and stage layer. 
Stage layer is used to process data for destination layer with additional transformation.

.. figure:: Mapping/csvIngestion.jpg
   :align: center

Mapping between source system and snowflake for these files is available   `Academic institute layout`_ 

.. _Academic institute layout:
    Mapping/AcademicInstitute.htm


Enrollment 
------------------

JSON is an open standard file format, and data interchange format, that uses human-readable text to store and transmit data objects consisting of attribute–value pairs and array data types. It is a very common data format, with a diverse range of applications, such as serving as a replacement for XML in AJAX systems.
In our use case – we are assuming Enrollment files are ingested (EFFY files) in Json format directly to snowflake.  
ELT layer defines the mapped attributes for Json attributes with data types. The following diagram displays mapping between Json attribute and stage layer. Stage layer is used to process data for destination layer with additional transformation.

.. figure:: Mapping/jsonIngestion.jpg
   :align: center

Mapping between source system and snowflake for these files is available   `Enrollment layout`_ 

.. _Enrollment layout:
    Mapping/Enrollment.htm



Institutional Charges 
---------------------
The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. Using ORC files improves performance when Hive is reading, writing, and processing data. It ideally stores data compact and enables skipping over irrelevant parts without the need for large, complex, or manually maintained indices.
In our use case – we are assuming Institutional Charge text files are ingested (ICAY files) in HDFS. They are processed through hive. Output of the big data processing (ORC files) are used to load in snowflake.
ELT layer defines the schema for ORC file with data types. The following diagram displays mapping between ORC attribute and stage layer. Stage layer is used to process data for destination layer with additional transformation and normalization.


.. figure:: Mapping/orcIngestion.jpg
   :align: center

Mapping between source system and snowflake for these files is available   `Institution charges layout`_ 

.. _Institution charges layout:
    Mapping/InstitutionCharges.htm


Admission data
---------------------
Parquet, an open-source file format for Hadoop stores nested data structures in a flat columnar format. Compared to a traditional approach where data is stored in a row-oriented approach, parquet is more efficient in terms of storage and performance. It is especially good for queries that read specific columns from a wide table. Parquet provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON, supported by many data processing systems. Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. The following diagram lays out the architecture from data source to parquet file.
In our use case – we are assuming - IPEDs admission data text files are ingested (ADM files) in HDFS and processed thru Spark. Output of the spark processing (parquet files) are used to process in snowflake.
ELT layer defines the schema for parquet file with data types. The following diagram displays mapping between parquet attribute and stage layer. Stage layer is used to process data for final destination layer with additional transformation.

.. figure:: Mapping/parquetIngestion.jpg
   :align: center

Mapping between source system and snowflake for these files is available   `Admission stat layout`_ 

.. _Admission stat layout:
    Mapping/AdmissionStat.htm


Reference/lookup data
---------------------
Code value lookup files are CSV format. These files are directly ingested into snowflake staging from source system. 
ELT layer defines the schema for CSV file with data types. 

Mapping between source system and snowflake for these files is available   `Lookup data layout`_ 

.. _Lookup data layout:
    Mapping/CodeValue.htm
